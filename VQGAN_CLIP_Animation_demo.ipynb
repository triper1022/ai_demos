{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m9bZg9zMTlZW"
      },
      "source": [
        "論文  \n",
        "https://arxiv.org/abs/2012.09841  \n",
        "  \n",
        "GitHub  \n",
        "https://github.com/chigozienri/VQGAN-CLIP-animations  \n",
        "  \n",
        "<a href=\"https://colab.research.google.com/github/kaz12tech/ai_demos/blob/master/VQGAN_CLIP_Animation_demo.ipynb\" target=\"_blank\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ETA7G8-9TlZc"
      },
      "source": [
        "# ランタイムの設定\n",
        "「ランタイム」→「ランタイムのタイプを変更」→「ハードウェアアクセラレータ」をGPUに変更"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EC0ghLSMTlZd"
      },
      "source": [
        "# 実行方法\n",
        "「ランタイム」→「すべてのセルを実行」を選択"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wYSww9QJTlZe"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5rRaifqlTlZf"
      },
      "source": [
        "# Google Driveのマウント"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0m8GaYEXTlZg"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Workspace作成"
      ],
      "metadata": {
        "id": "n-KIHtUwVfyc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir '/content/drive/MyDrive/vqgan'\n",
        "!mkdir '/content/drive/MyDrive/vqgan/images'\n",
        "working_dir = '/content/drive/MyDrive/vqgan'"
      ],
      "metadata": {
        "id": "VHXOUm7wVipy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xOAJsAW3TlZg"
      },
      "source": [
        "# ライブラリのインストール"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jcZzKaXGTlZh"
      },
      "outputs": [],
      "source": [
        "%cd /content/\n",
        "\n",
        "print(\"Downloading CLIP...\")\n",
        "!git clone https://github.com/openai/CLIP                 &> /dev/null\n",
        "\n",
        "print(\"Downloading Python AI libraries...\")\n",
        "!git clone https://github.com/CompVis/taming-transformers &> /dev/null\n",
        "!pip install ftfy regex tqdm omegaconf pytorch-lightning  &> /dev/null\n",
        "!pip install kornia                                       &> /dev/null\n",
        "!pip install einops                                       &> /dev/null\n",
        " \n",
        "print(\"Installing libraries for handling metadata...\")\n",
        "!pip install stegano                                      &> /dev/null\n",
        "!apt install exempi                                       &> /dev/null\n",
        "!pip install python-xmp-toolkit                           &> /dev/null\n",
        "!pip install imgtag                                       &> /dev/null\n",
        "!pip install pillow==7.1.2                                &> /dev/null\n",
        " \n",
        "print(\"Installing Python video creation libraries...\")\n",
        "!pip install imageio-ffmpeg &> /dev/null\n",
        "path = f'{working_dir}/steps'\n",
        "!mkdir --parents {path}\n",
        "print(\"Installation finished.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SbbO21fkTlZi"
      },
      "source": [
        "# ライブラリのインポート"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G7I23N9ETlZj"
      },
      "outputs": [],
      "source": [
        "import argparse\n",
        "import math\n",
        "from pathlib import Path\n",
        "import sys\n",
        "import os\n",
        "import cv2\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import subprocess\n",
        "import ast\n",
        " \n",
        "sys.path.append('/content/taming-transformers')\n",
        "\n",
        "# Some models include transformers, others need explicit pip install\n",
        "try:\n",
        "    import transformers\n",
        "except Exception:\n",
        "    !pip install transformers\n",
        "    import transformers\n",
        "\n",
        "from IPython import display\n",
        "from base64 import b64encode\n",
        "from omegaconf import OmegaConf\n",
        "from PIL import Image\n",
        "from taming.models import cond_transformer, vqgan\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.nn import functional as F\n",
        "from torchvision import transforms\n",
        "from torchvision.transforms import functional as TF\n",
        "from tqdm.notebook import tqdm\n",
        " \n",
        "from CLIP import clip\n",
        "import kornia.augmentation as K\n",
        "import numpy as np\n",
        "import imageio\n",
        "from PIL import ImageFile, Image\n",
        "from imgtag import ImgTag    # metadata \n",
        "from libxmp import *         # metadata\n",
        "import libxmp                # metadata\n",
        "from stegano import lsb\n",
        "import json\n",
        "ImageFile.LOAD_TRUNCATED_IMAGES = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cmNW5jHVTlZk"
      },
      "source": [
        "# util関数定義"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Krsb69oUTlZl"
      },
      "outputs": [],
      "source": [
        "def sinc(x):\n",
        "    return torch.where(x != 0, torch.sin(math.pi * x) / (math.pi * x), x.new_ones([]))\n",
        " \n",
        " \n",
        "def lanczos(x, a):\n",
        "    cond = torch.logical_and(-a < x, x < a)\n",
        "    out = torch.where(cond, sinc(x) * sinc(x/a), x.new_zeros([]))\n",
        "    return out / out.sum()\n",
        " \n",
        " \n",
        "def ramp(ratio, width):\n",
        "    n = math.ceil(width / ratio + 1)\n",
        "    out = torch.empty([n])\n",
        "    cur = 0\n",
        "    for i in range(out.shape[0]):\n",
        "        out[i] = cur\n",
        "        cur += ratio\n",
        "    return torch.cat([-out[1:].flip([0]), out])[1:-1]\n",
        " \n",
        " \n",
        "def resample(input, size, align_corners=True):\n",
        "    n, c, h, w = input.shape\n",
        "    dh, dw = size\n",
        " \n",
        "    input = input.view([n * c, 1, h, w])\n",
        " \n",
        "    if dh < h:\n",
        "        kernel_h = lanczos(ramp(dh / h, 2), 2).to(input.device, input.dtype)\n",
        "        pad_h = (kernel_h.shape[0] - 1) // 2\n",
        "        input = F.pad(input, (0, 0, pad_h, pad_h), 'reflect')\n",
        "        input = F.conv2d(input, kernel_h[None, None, :, None])\n",
        " \n",
        "    if dw < w:\n",
        "        kernel_w = lanczos(ramp(dw / w, 2), 2).to(input.device, input.dtype)\n",
        "        pad_w = (kernel_w.shape[0] - 1) // 2\n",
        "        input = F.pad(input, (pad_w, pad_w, 0, 0), 'reflect')\n",
        "        input = F.conv2d(input, kernel_w[None, None, None, :])\n",
        " \n",
        "    input = input.view([n, c, h, w])\n",
        "    return F.interpolate(input, size, mode='bicubic', align_corners=align_corners)\n",
        " \n",
        " \n",
        "class ReplaceGrad(torch.autograd.Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, x_forward, x_backward):\n",
        "        ctx.shape = x_backward.shape\n",
        "        return x_forward\n",
        " \n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_in):\n",
        "        return None, grad_in.sum_to_size(ctx.shape)\n",
        " \n",
        " \n",
        "replace_grad = ReplaceGrad.apply\n",
        " \n",
        " \n",
        "class ClampWithGrad(torch.autograd.Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, input, min, max):\n",
        "        ctx.min = min\n",
        "        ctx.max = max\n",
        "        ctx.save_for_backward(input)\n",
        "        return input.clamp(min, max)\n",
        " \n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_in):\n",
        "        input, = ctx.saved_tensors\n",
        "        return grad_in * (grad_in * (input - input.clamp(ctx.min, ctx.max)) >= 0), None, None\n",
        " \n",
        " \n",
        "clamp_with_grad = ClampWithGrad.apply\n",
        " \n",
        " \n",
        "def vector_quantize(x, codebook):\n",
        "    d = x.pow(2).sum(dim=-1, keepdim=True) + codebook.pow(2).sum(dim=1) - 2 * x @ codebook.T\n",
        "    indices = d.argmin(-1)\n",
        "    x_q = F.one_hot(indices, codebook.shape[0]).to(d.dtype) @ codebook\n",
        "    return replace_grad(x_q, x)\n",
        " \n",
        " \n",
        "class Prompt(nn.Module):\n",
        "    def __init__(self, embed, weight=1., stop=float('-inf')):\n",
        "        super().__init__()\n",
        "        self.register_buffer('embed', embed)\n",
        "        self.register_buffer('weight', torch.as_tensor(weight))\n",
        "        self.register_buffer('stop', torch.as_tensor(stop))\n",
        " \n",
        "    def forward(self, input):\n",
        "        input_normed = F.normalize(input.unsqueeze(1), dim=2)\n",
        "        embed_normed = F.normalize(self.embed.unsqueeze(0), dim=2)\n",
        "        dists = input_normed.sub(embed_normed).norm(dim=2).div(2).arcsin().pow(2).mul(2)\n",
        "        dists = dists * self.weight.sign()\n",
        "        return self.weight.abs() * replace_grad(dists, torch.maximum(dists, self.stop)).mean()\n",
        " \n",
        " \n",
        "def parse_prompt(prompt):\n",
        "    vals = prompt.rsplit(':', 2)\n",
        "    vals = vals + ['', '1', '-inf'][len(vals):]\n",
        "    return vals[0], float(vals[1]), float(vals[2])\n",
        " \n",
        " \n",
        "class MakeCutouts(nn.Module):\n",
        "    def __init__(self, cut_size, cutn, cut_pow=1.):\n",
        "        super().__init__()\n",
        "        self.cut_size = cut_size\n",
        "        self.cutn = cutn\n",
        "        self.cut_pow = cut_pow\n",
        "        self.augs = nn.Sequential(\n",
        "            K.RandomHorizontalFlip(p=0.5),\n",
        "            # K.RandomSolarize(0.01, 0.01, p=0.7),\n",
        "            K.RandomSharpness(0.3,p=0.4),\n",
        "            K.RandomAffine(degrees=30, translate=0.1, p=0.8, padding_mode='border'),\n",
        "            K.RandomPerspective(0.2,p=0.4),\n",
        "            K.ColorJitter(hue=0.01, saturation=0.01, p=0.7))\n",
        "        self.noise_fac = 0.1\n",
        " \n",
        " \n",
        "    def forward(self, input):\n",
        "        sideY, sideX = input.shape[2:4]\n",
        "        max_size = min(sideX, sideY)\n",
        "        min_size = min(sideX, sideY, self.cut_size)\n",
        "        cutouts = []\n",
        "        for _ in range(self.cutn):\n",
        "            size = int(torch.rand([])**self.cut_pow * (max_size - min_size) + min_size)\n",
        "            offsetx = torch.randint(0, sideX - size + 1, ())\n",
        "            offsety = torch.randint(0, sideY - size + 1, ())\n",
        "            cutout = input[:, :, offsety:offsety + size, offsetx:offsetx + size]\n",
        "            cutouts.append(resample(cutout, (self.cut_size, self.cut_size)))\n",
        "        batch = self.augs(torch.cat(cutouts, dim=0))\n",
        "        if self.noise_fac:\n",
        "            facs = batch.new_empty([self.cutn, 1, 1, 1]).uniform_(0, self.noise_fac)\n",
        "            batch = batch + facs * torch.randn_like(batch)\n",
        "        return batch\n",
        " \n",
        " \n",
        "def load_vqgan_model(config_path, checkpoint_path):\n",
        "    config = OmegaConf.load(config_path)\n",
        "    if config.model.target == 'taming.models.vqgan.VQModel':\n",
        "        model = vqgan.VQModel(**config.model.params)\n",
        "        model.eval().requires_grad_(False)\n",
        "        model.init_from_ckpt(checkpoint_path)\n",
        "    elif config.model.target == 'taming.models.cond_transformer.Net2NetTransformer':\n",
        "        parent_model = cond_transformer.Net2NetTransformer(**config.model.params)\n",
        "        parent_model.eval().requires_grad_(False)\n",
        "        parent_model.init_from_ckpt(checkpoint_path)\n",
        "        model = parent_model.first_stage_model\n",
        "    else:\n",
        "        raise ValueError(f'unknown model type: {config.model.target}')\n",
        "    del model.loss\n",
        "    return model\n",
        " \n",
        " \n",
        "def resize_image(image, out_size):\n",
        "    ratio = image.size[0] / image.size[1]\n",
        "    area = min(image.size[0] * image.size[1], out_size[0] * out_size[1])\n",
        "    size = round((area * ratio)**0.5), round((area / ratio)**0.5)\n",
        "    return image.resize(size, Image.LANCZOS)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# default画像取得\n",
        "デフォルトで使用するinitial_image, target_imageを取得"
      ],
      "metadata": {
        "id": "NOjMFXePbNLj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/vqgan/images\n",
        "!wget https://www.pakutaso.com/shared/img/thumb/nantoshi21PAR519902088_TP_V4.jpg\n",
        "!wget https://www.pakutaso.com/shared/img/thumb/yuka16011215IMG_5574_TP_V4.jpg\n",
        "\n",
        "src_img = Image.open('/content/drive/MyDrive/vqgan/images/nantoshi21PAR519902088_TP_V4.jpg')\n",
        "dst_img = Image.open('/content/drive/MyDrive/vqgan/images/yuka16011215IMG_5574_TP_V4.jpg')\n",
        "src_img = src_img.resize((src_img.width // 2, src_img.height // 2))\n",
        "dst_img = dst_img.resize((dst_img.width // 2, dst_img.height // 2))\n",
        "src_img.save('/content/drive/MyDrive/vqgan/images/nantoshi21PAR519902088_TP_V4.jpg')\n",
        "dst_img.save('/content/drive/MyDrive/vqgan/images/yuka16011215IMG_5574_TP_V4.jpg')\n",
        "\n",
        "\n",
        "%cd /content/"
      ],
      "metadata": {
        "id": "vFoYAvn0bdZR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2X1-S6TdTlZm"
      },
      "source": [
        "# パラメータ設定\n",
        "\n",
        "\n",
        "| Parameter  |  Usage |\n",
        "|---|---|\n",
        "| `key_frames` | 実行中にキーフレームを使用してパラメータを変更するかどうか |\n",
        "|  `text_prompts` |  テキストプロンプト (E.g. Apple': {10: 1, 20: 0}, 'Orange': {10: 0, 20: 1} Appleは10フレーム目で最大、20フレームで最小)|\n",
        "| `width` | 出力の幅（ピクセル単位）。 これは16の倍数に切り捨てられます |\n",
        "| `height` | 出力の高さ（ピクセル単位）。 これは16の倍数に切り捨てられます |\n",
        "| `model` | モデルの選択、上記からダウンロードする必要があります |\n",
        "| `interval` | ノートブックにフレームを表示する頻度（実際の出力には影響しません） |\n",
        "| `initial_image` | 開始する画像（ファイルへの相対パス）(E.g. ./content/src.jpg) |\n",
        "| `target_images` | ターゲットへの画像プロンプト(ファイルへの相対パス)(E.g. './content/init.jpg': {0: 1, 10: 0}, './content/final.jpg': {0: 0, 10: 1}) |\n",
        "| `seed` | ランダムシード。正の整数に設定されている場合、実行は繰り返し可能です（-1に設定されている場合、毎回同じ入力に対して同じ出力を取得し、ランダムシードが使用されます。 |\n",
        "| `max_frames` | アニメーションのフレーム数 |\n",
        "| `angle` | 各フレーム間で時計回りに回転する角度(度単位)(E.g. 10: 0, 30: 1, 50: -1) |\n",
        "| `zoom` | 各フレームをズームインするための係数、1はズームなし、1未満はズームアウト、1を超える場合はズームイン(正の値のみ)(E.g. 10: 1, 30: 1.2, 50: 0.9) |\n",
        "| `translation_x` | 各フレームを右にシフトするピクセル数 |\n",
        "| `translation_y` | 各フレームを下にシフトするピクセル数 |\n",
        "| `iterations_per_frame` | 各フレームでVQGAN + CLIPメソッドを実行する回数 |\n",
        "| `save_all_iterations` | デバッグ中、通常の操作ではFalseに設定 |\n",
        "\n",
        "---------\n",
        "\n",
        "多数のキーフレームを使用してアニメーションを作成する場合は、@ EphemeralIncによるこのスプレッドシートを試して、文字列を作成してください：  \n",
        "https：//docs.google.com/spreadsheets/d/1sJ0PMHUPIYkS7LSxhzTThEP7rZ5CFonz-dBxqe8F2uc。\n",
        "  \n",
        "https://keyframe-string-generator.glitch.me/  \n",
        "または  \n",
        "https://audio-keyframe-generator.glitch.me/  \n",
        "を試して、ビジュアルエディターまたはオーディオファイルを使用して文字列を作成することもできます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JxBfMGmNTlZn"
      },
      "outputs": [],
      "source": [
        "key_frames = True #@param {type:\"boolean\"}\n",
        "text_prompts = \"'Moon': {10: 0, 60: 1}, 'Sun': {10: 1, 60: 0}\" #@param {type:\"string\"}\n",
        "width =  400 #@param {type:\"number\"}\n",
        "height =  400 #@param {type:\"number\"}\n",
        "model = \"vqgan_imagenet_f16_16384\" #@param [\"vqgan_imagenet_f16_16384\", \"vqgan_imagenet_f16_1024\", \"wikiart_16384\", \"coco\", \"faceshq\", \"sflckr\"]\n",
        "interval =  1#@param {type:\"number\"}\n",
        "initial_image = \"/content/drive/MyDrive/vqgan/images/nantoshi21PAR519902088_TP_V4.jpg\"#@param {type:\"string\"}\n",
        "target_images = \"'/content/drive/MyDrive/vqgan/images/yuka16011215IMG_5574_TP_V4.jpg': {10: 0, 60: 1}\"#@param {type:\"string\"}\n",
        "seed = 1#@param {type:\"number\"}\n",
        "max_frames = 60#@param {type:\"number\"}\n",
        "angle = \"10: 0, 30: 1, 60: -1\"#@param {type:\"string\"}\n",
        "\n",
        "# @markdown <b>Careful:</b> do not use negative or 0 zoom. If you want to zoom out, use a number between 0 and 1.\n",
        "zoom = \"10: 1, 30: 1.2, 60: 0.9\"#@param {type:\"string\"}\n",
        "translation_x = \"0: 0\"#@param {type:\"string\"}\n",
        "translation_y = \"0: 0\"#@param {type:\"string\"}\n",
        "iterations_per_frame = \"0: 10\"#@param {type:\"string\"}\n",
        "save_all_iterations = False#@param {type:\"boolean\"}\n",
        "\n",
        "# option -C - skips download if already exists\n",
        "!curl -C - -L -o {model}.yaml -C - 'https://heibox.uni-heidelberg.de/d/8088892a516d4e3baf92/files/?p=%2Fconfigs%2Fmodel.yaml&dl=1' #ImageNet 1024\n",
        "!curl -C - -L -o {model}.ckpt -C - 'https://heibox.uni-heidelberg.de/d/8088892a516d4e3baf92/files/?p=%2Fckpts%2Flast.ckpt&dl=1'  #ImageNet 1024\n",
        "\n",
        "if initial_image != \"\":\n",
        "    print(\n",
        "        \"WARNING: You have specified an initial image. Note that the image resolution \"\n",
        "        \"will be inherited from this image, not whatever width and height you specified. \"\n",
        "        \"If the initial image resolution is too high, this can result in out of memory errors.\"\n",
        "    )\n",
        "elif width * height > 160000:\n",
        "    print(\n",
        "        \"WARNING: The width and height you have specified may be too high, in which case \"\n",
        "        \"you will encounter out of memory errors either at the image generation stage or the \"\n",
        "        \"video synthesis stage. If so, try reducing the resolution\"\n",
        "    )\n",
        "model_names={\n",
        "    \"vqgan_imagenet_f16_16384\": 'ImageNet 16384',\n",
        "    \"vqgan_imagenet_f16_1024\":\"ImageNet 1024\", \n",
        "    \"wikiart_1024\":\"WikiArt 1024\",\n",
        "    \"wikiart_16384\":\"WikiArt 16384\",\n",
        "    \"coco\":\"COCO-Stuff\",\n",
        "    \"faceshq\":\"FacesHQ\",\n",
        "    \"sflckr\":\"S-FLCKR\"\n",
        "}\n",
        "model_name = model_names[model]\n",
        "\n",
        "if seed == -1:\n",
        "    seed = None\n",
        "\n",
        "def parse_key_frames(string, prompt_parser=None):\n",
        "    \"\"\"Given a string representing frame numbers paired with parameter values at that frame,\n",
        "    return a dictionary with the frame numbers as keys and the parameter values as the values.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    string: string\n",
        "        Frame numbers paired with parameter values at that frame number, in the format\n",
        "        'framenumber1: (parametervalues1), framenumber2: (parametervalues2), ...'\n",
        "    prompt_parser: function or None, optional\n",
        "        If provided, prompt_parser will be applied to each string of parameter values.\n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "    dict\n",
        "        Frame numbers as keys, parameter values at that frame number as values\n",
        "\n",
        "    Raises\n",
        "    ------\n",
        "    RuntimeError\n",
        "        If the input string does not match the expected format.\n",
        "    \n",
        "    Examples\n",
        "    --------\n",
        "    >>> parse_key_frames(\"10:(Apple: 1| Orange: 0), 20: (Apple: 0| Orange: 1| Peach: 1)\")\n",
        "    {10: 'Apple: 1| Orange: 0', 20: 'Apple: 0| Orange: 1| Peach: 1'}\n",
        "\n",
        "    >>> parse_key_frames(\"10:(Apple: 1| Orange: 0), 20: (Apple: 0| Orange: 1| Peach: 1)\", prompt_parser=lambda x: x.lower()))\n",
        "    {10: 'apple: 1| orange: 0', 20: 'apple: 0| orange: 1| peach: 1'}\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "        # This is the preferred way, the regex way will eventually be deprecated.\n",
        "        frames = ast.literal_eval('{' + string + '}')\n",
        "        if isinstance(frames, set):\n",
        "            # If user forgot keyframes, just set value of frame 0\n",
        "            (frame,) = list(frames)\n",
        "            frames = {0: frame}\n",
        "        return frames\n",
        "    except Exception:\n",
        "        import re\n",
        "        pattern = r'((?P<frame>[0-9]+):[\\s]*[\\(](?P<param>[\\S\\s]*?)[\\)])'\n",
        "        frames = dict()\n",
        "        for match_object in re.finditer(pattern, string):\n",
        "            frame = int(match_object.groupdict()['frame'])\n",
        "            param = match_object.groupdict()['param']\n",
        "            if prompt_parser:\n",
        "                frames[frame] = prompt_parser(param)\n",
        "            else:\n",
        "                frames[frame] = param\n",
        "\n",
        "        if frames == {} and len(string) != 0:\n",
        "            raise RuntimeError(f'Key Frame string not correctly formatted: {string}')\n",
        "        return frames\n",
        "\n",
        "# Defaults, if left empty\n",
        "if angle == \"\":\n",
        "    angle = \"0\"\n",
        "if zoom == \"\":\n",
        "    zoom = \"1\"\n",
        "if translation_x == \"\":\n",
        "    translation_x = \"0\"\n",
        "if translation_y == \"\":\n",
        "    translation_y = \"0\"\n",
        "if iterations_per_frame == \"\":\n",
        "    iterations_per_frame = \"10\"\n",
        "\n",
        "if key_frames:\n",
        "    parameter_dicts = dict()\n",
        "    parameter_dicts['zoom'] = parse_key_frames(zoom, prompt_parser=float)\n",
        "    parameter_dicts['angle'] = parse_key_frames(angle, prompt_parser=float)\n",
        "    parameter_dicts['translation_x'] = parse_key_frames(translation_x, prompt_parser=float)\n",
        "    parameter_dicts['translation_y'] = parse_key_frames(translation_y, prompt_parser=float)\n",
        "    parameter_dicts['iterations_per_frame'] = parse_key_frames(iterations_per_frame, prompt_parser=int)\n",
        "\n",
        "    text_prompts_dict = parse_key_frames(text_prompts)\n",
        "    if all([isinstance(value, dict) for value in list(text_prompts_dict.values())]):\n",
        "       for key, value in list(text_prompts_dict.items()):\n",
        "           parameter_dicts[f'text_prompt: {key}'] = value\n",
        "    else:\n",
        "        # Old format\n",
        "        text_prompts_dict = parse_key_frames(text_prompts, prompt_parser=lambda x: x.split('|'))\n",
        "        for frame, prompt_list in text_prompts_dict.items():\n",
        "            for prompt in prompt_list:\n",
        "                prompt_key, prompt_value = prompt.split(\":\")\n",
        "                prompt_key = f'text_prompt: {prompt_key.strip()}'\n",
        "                prompt_value = prompt_value.strip()\n",
        "                if prompt_key not in parameter_dicts:\n",
        "                    parameter_dicts[prompt_key] = dict()\n",
        "                parameter_dicts[prompt_key][frame] = prompt_value\n",
        "\n",
        "\n",
        "    image_prompts_dict = parse_key_frames(target_images)\n",
        "    if all([isinstance(value, dict) for value in list(image_prompts_dict.values())]):\n",
        "        for key, value in list(image_prompts_dict.items()):\n",
        "           parameter_dicts[f'image_prompt: {key}'] = value\n",
        "    else:\n",
        "        # Old format\n",
        "        image_prompts_dict = parse_key_frames(target_images, prompt_parser=lambda x: x.split('|'))\n",
        "        for frame, prompt_list in image_prompts_dict.items():\n",
        "            for prompt in prompt_list:\n",
        "                prompt_key, prompt_value = prompt.split(\":\")\n",
        "                prompt_key = f'image_prompt: {prompt_key.strip()}'\n",
        "                prompt_value = prompt_value.strip()\n",
        "                if prompt_key not in parameter_dicts:\n",
        "                    parameter_dicts[prompt_key] = dict()\n",
        "                parameter_dicts[prompt_key][frame] = prompt_value\n",
        "\n",
        "\n",
        "def add_inbetweens():\n",
        "    global text_prompts\n",
        "    global target_images\n",
        "    global zoom\n",
        "    global angle\n",
        "    global translation_x\n",
        "    global translation_y\n",
        "    global iterations_per_frame\n",
        "\n",
        "    global text_prompts_series\n",
        "    global target_images_series\n",
        "    global zoom_series\n",
        "    global angle_series\n",
        "    global translation_x_series\n",
        "    global translation_y_series\n",
        "    global iterations_per_frame_series\n",
        "    global model\n",
        "    global args\n",
        "    def get_inbetweens(key_frames_dict, integer=False):\n",
        "        \"\"\"Given a dict with frame numbers as keys and a parameter value as values,\n",
        "        return a pandas Series containing the value of the parameter at every frame from 0 to max_frames.\n",
        "        Any values not provided in the input dict are calculated by linear interpolation between\n",
        "        the values of the previous and next provided frames. If there is no previous provided frame, then\n",
        "        the value is equal to the value of the next provided frame, or if there is no next provided frame,\n",
        "        then the value is equal to the value of the previous provided frame. If no frames are provided,\n",
        "        all frame values are NaN.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        key_frames_dict: dict\n",
        "            A dict with integer frame numbers as keys and numerical values of a particular parameter as values.\n",
        "        integer: Bool, optional\n",
        "            If True, the values of the output series are converted to integers.\n",
        "            Otherwise, the values are floats.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        pd.Series\n",
        "            A Series with length max_frames representing the parameter values for each frame.\n",
        "        \n",
        "        Examples\n",
        "        --------\n",
        "        >>> max_frames = 5\n",
        "        >>> get_inbetweens({1: 5, 3: 6})\n",
        "        0    5.0\n",
        "        1    5.0\n",
        "        2    5.5\n",
        "        3    6.0\n",
        "        4    6.0\n",
        "        dtype: float64\n",
        "\n",
        "        >>> get_inbetweens({1: 5, 3: 6}, integer=True)\n",
        "        0    5\n",
        "        1    5\n",
        "        2    5\n",
        "        3    6\n",
        "        4    6\n",
        "        dtype: int64\n",
        "        \"\"\"\n",
        "        key_frame_series = pd.Series([np.nan for a in range(max_frames)])\n",
        "        for i, value in key_frames_dict.items():\n",
        "            key_frame_series[i] = value\n",
        "        key_frame_series = key_frame_series.astype(float)\n",
        "        key_frame_series = key_frame_series.interpolate(limit_direction='both')\n",
        "        if integer:\n",
        "            return key_frame_series.astype(int)\n",
        "        return key_frame_series\n",
        "\n",
        "    if key_frames:\n",
        "        text_prompts_series_dict = dict()\n",
        "        for parameter in parameter_dicts.keys():\n",
        "            if len(parameter_dicts[parameter]) > 0:\n",
        "                if parameter.startswith('text_prompt:'):\n",
        "                    try:\n",
        "                        text_prompts_series_dict[parameter] = get_inbetweens(parameter_dicts[parameter])\n",
        "                    except RuntimeError as e:\n",
        "                        raise RuntimeError(\n",
        "                            \"WARNING: You have selected to use key frames, but you have not \"\n",
        "                            \"formatted `text_prompts` correctly for key frames.\\n\"\n",
        "                            \"Please read the instructions to find out how to use key frames \"\n",
        "                            \"correctly.\\n\"\n",
        "                        )\n",
        "        text_prompts_series = pd.Series([np.nan for a in range(max_frames)])\n",
        "        for i in range(max_frames):\n",
        "            combined_prompt = []\n",
        "            for parameter, value in text_prompts_series_dict.items():\n",
        "                parameter = parameter[len('text_prompt:'):].strip()\n",
        "                combined_prompt.append(f'{parameter}: {value[i]}')\n",
        "            text_prompts_series[i] = ' | '.join(combined_prompt)\n",
        "\n",
        "        image_prompts_series_dict = dict()\n",
        "        for parameter in parameter_dicts.keys():\n",
        "            if len(parameter_dicts[parameter]) > 0:\n",
        "                if parameter.startswith('image_prompt:'):\n",
        "                    try:\n",
        "                        image_prompts_series_dict[parameter] = get_inbetweens(parameter_dicts[parameter])\n",
        "                    except RuntimeError as e:\n",
        "                        raise RuntimeError(\n",
        "                            \"WARNING: You have selected to use key frames, but you have not \"\n",
        "                            \"formatted `image_prompts` correctly for key frames.\\n\"\n",
        "                            \"Please read the instructions to find out how to use key frames \"\n",
        "                            \"correctly.\\n\"\n",
        "                        )\n",
        "        target_images_series = pd.Series([np.nan for a in range(max_frames)])\n",
        "        for i in range(max_frames):\n",
        "            combined_prompt = []\n",
        "            for parameter, value in image_prompts_series_dict.items():\n",
        "                parameter = parameter[len('image_prompt:'):].strip()\n",
        "                combined_prompt.append(f'{parameter}: {value[i]}')\n",
        "            target_images_series[i] = ' | '.join(combined_prompt)\n",
        "\n",
        "        try:\n",
        "            angle_series = get_inbetweens(parameter_dicts['angle'])\n",
        "        except RuntimeError as e:\n",
        "            print(\n",
        "                \"WARNING: You have selected to use key frames, but you have not \"\n",
        "                \"formatted `angle` correctly for key frames.\\n\"\n",
        "                \"Attempting to interpret `angle` as \"\n",
        "                f'\"0: ({angle})\"\\n'\n",
        "                \"Please read the instructions to find out how to use key frames \"\n",
        "                \"correctly.\\n\"\n",
        "            )\n",
        "            angle = f\"0: ({angle})\"\n",
        "            angle_series = get_inbetweens(parse_key_frames(angle))\n",
        "\n",
        "        try:\n",
        "            zoom_series = get_inbetweens(parameter_dicts['zoom'])\n",
        "        except RuntimeError as e:\n",
        "            print(\n",
        "                \"WARNING: You have selected to use key frames, but you have not \"\n",
        "                \"formatted `zoom` correctly for key frames.\\n\"\n",
        "                \"Attempting to interpret `zoom` as \"\n",
        "                f'\"0: ({zoom})\"\\n'\n",
        "                \"Please read the instructions to find out how to use key frames \"\n",
        "                \"correctly.\\n\"\n",
        "            )\n",
        "            zoom = f\"0: ({zoom})\"\n",
        "            zoom_series = get_inbetweens(parse_key_frames(zoom))\n",
        "        for i, zoom in enumerate(zoom_series):\n",
        "            if zoom <= 0:\n",
        "                print(\n",
        "                    f\"WARNING: You have selected a zoom of {zoom} at frame {i}. \"\n",
        "                    \"This is meaningless. \"\n",
        "                    \"If you want to zoom out, use a value between 0 and 1. \"\n",
        "                    \"If you want no zoom, use a value of 1.\"\n",
        "                )\n",
        "\n",
        "        try:\n",
        "            translation_x_series = get_inbetweens(parameter_dicts['translation_x'])\n",
        "        except RuntimeError as e:\n",
        "            print(\n",
        "                \"WARNING: You have selected to use key frames, but you have not \"\n",
        "                \"formatted `translation_x` correctly for key frames.\\n\"\n",
        "                \"Attempting to interpret `translation_x` as \"\n",
        "                f'\"0: ({translation_x})\"\\n'\n",
        "                \"Please read the instructions to find out how to use key frames \"\n",
        "                \"correctly.\\n\"\n",
        "            )\n",
        "            translation_x = f\"0: ({translation_x})\"\n",
        "            translation_x_series = get_inbetweens(parse_key_frames(translation_x))\n",
        "\n",
        "        try:\n",
        "            translation_y_series = get_inbetweens(parameter_dicts['translation_y'])\n",
        "        except RuntimeError as e:\n",
        "            print(\n",
        "                \"WARNING: You have selected to use key frames, but you have not \"\n",
        "                \"formatted `translation_y` correctly for key frames.\\n\"\n",
        "                \"Attempting to interpret `translation_y` as \"\n",
        "                f'\"0: ({translation_y})\"\\n'\n",
        "                \"Please read the instructions to find out how to use key frames \"\n",
        "                \"correctly.\\n\"\n",
        "            )\n",
        "            translation_y = f\"0: ({translation_y})\"\n",
        "            translation_y_series = get_inbetweens(parse_key_frames(translation_y))\n",
        "\n",
        "        try:\n",
        "            iterations_per_frame_series = get_inbetweens(\n",
        "                parameter_dicts['iterations_per_frame'], integer=True\n",
        "            )\n",
        "        except RuntimeError as e:\n",
        "            print(\n",
        "                \"WARNING: You have selected to use key frames, but you have not \"\n",
        "                \"formatted `iterations_per_frame` correctly for key frames.\\n\"\n",
        "                \"Attempting to interpret `iterations_per_frame` as \"\n",
        "                f'\"0: ({iterations_per_frame})\"\\n'\n",
        "                \"Please read the instructions to find out how to use key frames \"\n",
        "                \"correctly.\\n\"\n",
        "            )\n",
        "            iterations_per_frame = f\"0: ({iterations_per_frame})\"\n",
        "            \n",
        "            iterations_per_frame_series = get_inbetweens(\n",
        "                parse_key_frames(iterations_per_frame), integer=True\n",
        "            )\n",
        "    else:\n",
        "        text_prompts = [phrase.strip() for phrase in text_prompts.split(\"|\")]\n",
        "        if text_prompts == ['']:\n",
        "            text_prompts = []\n",
        "        if target_images == \"None\" or not target_images:\n",
        "            target_images = []\n",
        "        else:\n",
        "            target_images = target_images.split(\"|\")\n",
        "            target_images = [image.strip() for image in target_images]\n",
        "\n",
        "        angle = float(angle)\n",
        "        zoom = float(zoom)\n",
        "        translation_x = float(translation_x)\n",
        "        translation_y = float(translation_y)\n",
        "        iterations_per_frame = int(iterations_per_frame)\n",
        "        if zoom <= 0:\n",
        "            print(\n",
        "                f\"WARNING: You have selected a zoom of {zoom}. \"\n",
        "                \"This is meaningless. \"\n",
        "                \"If you want to zoom out, use a value between 0 and 1. \"\n",
        "                \"If you want no zoom, use a value of 1.\"\n",
        "            )\n",
        "\n",
        "    args = argparse.Namespace(\n",
        "        prompts=text_prompts,\n",
        "        image_prompts=target_images,\n",
        "        noise_prompt_seeds=[],\n",
        "        noise_prompt_weights=[],\n",
        "        size=[width, height],\n",
        "        init_weight=0.,\n",
        "        clip_model='ViT-B/32',\n",
        "        vqgan_config=f'{model}.yaml',\n",
        "        vqgan_checkpoint=f'{model}.ckpt',\n",
        "        step_size=0.1,\n",
        "        cutn=64,\n",
        "        cut_pow=1.,\n",
        "        display_freq=interval,\n",
        "        seed=seed,\n",
        "    )\n",
        "\n",
        "add_inbetweens()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "giVRFLJHTlZq"
      },
      "outputs": [],
      "source": [
        "path = f'{working_dir}/steps'\n",
        "!rm -r {path}\n",
        "!mkdir --parents {path}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tft1fUI5TlZs"
      },
      "outputs": [],
      "source": [
        "#@title Actually do the run...\n",
        "\n",
        "# Delete memory from previous runs\n",
        "!nvidia-smi -caa\n",
        "for var in ['device', 'model', 'perceptor', 'z']:\n",
        "  try:\n",
        "      del globals()[var]\n",
        "  except:\n",
        "      pass\n",
        "\n",
        "try:\n",
        "    import gc\n",
        "    gc.collect()\n",
        "except:\n",
        "    pass\n",
        "\n",
        "try:\n",
        "    torch.cuda.empty_cache()\n",
        "except:\n",
        "    pass\n",
        "\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "print('Using device:', device)\n",
        "if not key_frames:\n",
        "    if text_prompts:\n",
        "        print('Using text prompts:', text_prompts)\n",
        "    if target_images:\n",
        "        print('Using image prompts:', target_images)\n",
        "if args.seed is None:\n",
        "    seed = torch.seed()\n",
        "else:\n",
        "    seed = args.seed\n",
        "torch.manual_seed(seed)\n",
        "print('Using seed:', seed)\n",
        " \n",
        "model = load_vqgan_model(args.vqgan_config, args.vqgan_checkpoint).to(device)\n",
        "perceptor = clip.load(args.clip_model, jit=False)[0].eval().requires_grad_(False).to(device)\n",
        " \n",
        "cut_size = perceptor.visual.input_resolution\n",
        "e_dim = model.quantize.e_dim\n",
        "f = 2**(model.decoder.num_resolutions - 1)\n",
        "make_cutouts = MakeCutouts(cut_size, args.cutn, cut_pow=args.cut_pow)\n",
        "n_toks = model.quantize.n_e\n",
        "toksX, toksY = args.size[0] // f, args.size[1] // f\n",
        "sideX, sideY = toksX * f, toksY * f\n",
        "z_min = model.quantize.embedding.weight.min(dim=0).values[None, :, None, None]\n",
        "z_max = model.quantize.embedding.weight.max(dim=0).values[None, :, None, None]\n",
        "stop_on_next_loop = False  # Make sure GPU memory doesn't get corrupted from cancelling the run mid-way through, allow a full frame to complete\n",
        "\n",
        "def read_image_workaround(path):\n",
        "    \"\"\"OpenCV reads images as BGR, Pillow saves them as RGB. Work around\n",
        "    this incompatibility to avoid colour inversions.\"\"\"\n",
        "    im_tmp = cv2.imread(path)\n",
        "    return cv2.cvtColor(im_tmp, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "for i in range(max_frames):\n",
        "    if stop_on_next_loop:\n",
        "      break\n",
        "    if key_frames:\n",
        "        text_prompts = text_prompts_series[i]\n",
        "        text_prompts = [phrase.strip() for phrase in text_prompts.split(\"|\")]\n",
        "        if text_prompts == ['']:\n",
        "            text_prompts = []\n",
        "        args.prompts = text_prompts\n",
        "\n",
        "        target_images = target_images_series[i]\n",
        "\n",
        "        if target_images == \"None\" or not target_images:\n",
        "            target_images = []\n",
        "        else:\n",
        "            target_images = target_images.split(\"|\")\n",
        "            target_images = [image.strip() for image in target_images]\n",
        "        args.image_prompts = target_images\n",
        "\n",
        "        angle = angle_series[i]\n",
        "        zoom = zoom_series[i]\n",
        "        translation_x = translation_x_series[i]\n",
        "        translation_y = translation_y_series[i]\n",
        "        iterations_per_frame = iterations_per_frame_series[i]\n",
        "        print(\n",
        "            f'text_prompts: {text_prompts}',\n",
        "            f'image_prompts: {target_images}',\n",
        "            f'angle: {angle}',\n",
        "            f'zoom: {zoom}',\n",
        "            f'translation_x: {translation_x}',\n",
        "            f'translation_y: {translation_y}',\n",
        "            f'iterations_per_frame: {iterations_per_frame}'\n",
        "        )\n",
        "    try:\n",
        "        if i == 0 and initial_image != \"\":\n",
        "            img_0 = read_image_workaround(initial_image)\n",
        "            z, *_ = model.encode(TF.to_tensor(img_0).to(device).unsqueeze(0) * 2 - 1)\n",
        "        elif i == 0 and not os.path.isfile(f'{working_dir}/steps/{i:04d}.png'):\n",
        "            one_hot = F.one_hot(\n",
        "                torch.randint(n_toks, [toksY * toksX], device=device), n_toks\n",
        "            ).float()\n",
        "            z = one_hot @ model.quantize.embedding.weight\n",
        "            z = z.view([-1, toksY, toksX, e_dim]).permute(0, 3, 1, 2)\n",
        "        else:\n",
        "            if save_all_iterations:\n",
        "                img_0 = read_image_workaround(\n",
        "                    f'{working_dir}/steps/{i:04d}_{iterations_per_frame}.png')\n",
        "            else:\n",
        "                img_0 = read_image_workaround(f'{working_dir}/steps/{i:04d}.png')\n",
        "\n",
        "            center = (1*img_0.shape[1]//2, 1*img_0.shape[0]//2)\n",
        "            trans_mat = np.float32(\n",
        "                [[1, 0, translation_x],\n",
        "                [0, 1, translation_y]]\n",
        "            )\n",
        "            rot_mat = cv2.getRotationMatrix2D( center, angle, zoom )\n",
        "\n",
        "            trans_mat = np.vstack([trans_mat, [0,0,1]])\n",
        "            rot_mat = np.vstack([rot_mat, [0,0,1]])\n",
        "            transformation_matrix = np.matmul(rot_mat, trans_mat)\n",
        "\n",
        "            img_0 = cv2.warpPerspective(\n",
        "                img_0,\n",
        "                transformation_matrix,\n",
        "                (img_0.shape[1], img_0.shape[0]),\n",
        "                borderMode=cv2.BORDER_WRAP\n",
        "            )\n",
        "            z, *_ = model.encode(TF.to_tensor(img_0).to(device).unsqueeze(0) * 2 - 1)\n",
        "        i += 1\n",
        "\n",
        "        z_orig = z.clone()\n",
        "        z.requires_grad_(True)\n",
        "        opt = optim.Adam([z], lr=args.step_size)\n",
        "\n",
        "        normalize = transforms.Normalize(mean=[0.48145466, 0.4578275, 0.40821073],\n",
        "                                        std=[0.26862954, 0.26130258, 0.27577711])\n",
        "\n",
        "        pMs = []\n",
        "\n",
        "        for prompt in args.prompts:\n",
        "            txt, weight, stop = parse_prompt(prompt)\n",
        "            embed = perceptor.encode_text(clip.tokenize(txt).to(device)).float()\n",
        "            pMs.append(Prompt(embed, weight, stop).to(device))\n",
        "\n",
        "        for prompt in args.image_prompts:\n",
        "            path, weight, stop = parse_prompt(prompt)\n",
        "            img = resize_image(Image.open(path).convert('RGB'), (sideX, sideY))\n",
        "            batch = make_cutouts(TF.to_tensor(img).unsqueeze(0).to(device))\n",
        "            embed = perceptor.encode_image(normalize(batch)).float()\n",
        "            pMs.append(Prompt(embed, weight, stop).to(device))\n",
        "\n",
        "        for seed, weight in zip(args.noise_prompt_seeds, args.noise_prompt_weights):\n",
        "            gen = torch.Generator().manual_seed(seed)\n",
        "            embed = torch.empty([1, perceptor.visual.output_dim]).normal_(generator=gen)\n",
        "            pMs.append(Prompt(embed, weight).to(device))\n",
        "\n",
        "        def synth(z):\n",
        "            z_q = vector_quantize(z.movedim(1, 3), model.quantize.embedding.weight).movedim(3, 1)\n",
        "            return clamp_with_grad(model.decode(z_q).add(1).div(2), 0, 1)\n",
        "\n",
        "        def add_xmp_data(filename):\n",
        "            imagen = ImgTag(filename=filename)\n",
        "            imagen.xmp.append_array_item(libxmp.consts.XMP_NS_DC, 'creator', 'VQGAN+CLIP', {\"prop_array_is_ordered\":True, \"prop_value_is_array\":True})\n",
        "            if args.prompts:\n",
        "                imagen.xmp.append_array_item(libxmp.consts.XMP_NS_DC, 'title', \" | \".join(args.prompts), {\"prop_array_is_ordered\":True, \"prop_value_is_array\":True})\n",
        "            else:\n",
        "                imagen.xmp.append_array_item(libxmp.consts.XMP_NS_DC, 'title', 'None', {\"prop_array_is_ordered\":True, \"prop_value_is_array\":True})\n",
        "            imagen.xmp.append_array_item(libxmp.consts.XMP_NS_DC, 'i', str(i), {\"prop_array_is_ordered\":True, \"prop_value_is_array\":True})\n",
        "            imagen.xmp.append_array_item(libxmp.consts.XMP_NS_DC, 'model', model_name, {\"prop_array_is_ordered\":True, \"prop_value_is_array\":True})\n",
        "            imagen.xmp.append_array_item(libxmp.consts.XMP_NS_DC, 'seed',str(seed) , {\"prop_array_is_ordered\":True, \"prop_value_is_array\":True})\n",
        "            imagen.close()\n",
        "\n",
        "        def add_stegano_data(filename):\n",
        "            data = {\n",
        "                \"title\": \" | \".join(args.prompts) if args.prompts else None,\n",
        "                \"notebook\": \"VQGAN+CLIP\",\n",
        "                \"i\": i,\n",
        "                \"model\": model_name,\n",
        "                \"seed\": str(seed),\n",
        "            }\n",
        "            lsb.hide(filename, json.dumps(data)).save(filename)\n",
        "\n",
        "        @torch.no_grad()\n",
        "        def checkin(i, losses):\n",
        "            losses_str = ', '.join(f'{loss.item():g}' for loss in losses)\n",
        "            tqdm.write(f'i: {i}, loss: {sum(losses).item():g}, losses: {losses_str}')\n",
        "            out = synth(z)\n",
        "            TF.to_pil_image(out[0].cpu()).save('progress.png')\n",
        "            add_stegano_data('progress.png')\n",
        "            add_xmp_data('progress.png')\n",
        "            display.display(display.Image('progress.png'))\n",
        "\n",
        "        def save_output(i, img, suffix=None):\n",
        "            filename = \\\n",
        "                f\"{working_dir}/steps/{i:04}{'_' + suffix if suffix else ''}.png\"\n",
        "            imageio.imwrite(filename, np.array(img))\n",
        "            add_stegano_data(filename)\n",
        "            add_xmp_data(filename)\n",
        "\n",
        "        def ascend_txt(i, save=True, suffix=None):\n",
        "            out = synth(z)\n",
        "            iii = perceptor.encode_image(normalize(make_cutouts(out))).float()\n",
        "\n",
        "            result = []\n",
        "\n",
        "            if args.init_weight:\n",
        "                result.append(F.mse_loss(z, z_orig) * args.init_weight / 2)\n",
        "\n",
        "            for prompt in pMs:\n",
        "                result.append(prompt(iii))\n",
        "            img = np.array(out.mul(255).clamp(0, 255)[0].cpu().detach().numpy().astype(np.uint8))[:,:,:]\n",
        "            img = np.transpose(img, (1, 2, 0))\n",
        "            if save:\n",
        "                save_output(i, img, suffix=suffix)\n",
        "            return result\n",
        "\n",
        "        def train(i, save=True, suffix=None):\n",
        "            opt.zero_grad()\n",
        "            lossAll = ascend_txt(i, save=save, suffix=suffix)\n",
        "            if i % args.display_freq == 0 and save:\n",
        "                checkin(i, lossAll)\n",
        "            loss = sum(lossAll)\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "            with torch.no_grad():\n",
        "                z.copy_(z.maximum(z_min).minimum(z_max))\n",
        "\n",
        "        with tqdm() as pbar:\n",
        "            if iterations_per_frame == 0:\n",
        "                save_output(i, img_0)\n",
        "            j = 1\n",
        "            while True:\n",
        "                suffix = (str(j) if save_all_iterations else None)\n",
        "                if j >= iterations_per_frame:\n",
        "                    train(i, save=True, suffix=suffix)\n",
        "                    break\n",
        "                if save_all_iterations:\n",
        "                    train(i, save=True, suffix=suffix)\n",
        "                else:\n",
        "                    train(i, save=False, suffix=suffix)\n",
        "                j += 1\n",
        "                pbar.update()\n",
        "    except KeyboardInterrupt:\n",
        "      stop_on_next_loop = True\n",
        "      pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TO32fdaRTlZt"
      },
      "source": [
        "# SRCNNによる超解像(Option)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BJULk137TlZt"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/Mirwaisse/SRCNN.git\n",
        "!curl https://raw.githubusercontent.com/chigozienri/SRCNN/master/models/model_2x.pth -o model_2x.pth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i6FMD1bzTlZu"
      },
      "outputs": [],
      "source": [
        "# @title Increase Resolution\n",
        "\n",
        "# import subprocess in case this cell is run without the above cells\n",
        "import subprocess\n",
        "# Set zoomed = True if this cell is run\n",
        "zoomed = True\n",
        "\n",
        "init_frame = 1#@param {type:\"number\"}\n",
        "last_frame = 60#@param {type:\"number\"}\n",
        "\n",
        "for i in range(init_frame, last_frame + 1): #\n",
        "    filename = f\"{i:04}.png\"\n",
        "    cmd = [\n",
        "        'python',\n",
        "        '/content/SRCNN/run.py',\n",
        "        '--zoom_factor',\n",
        "        '2',  # Note if you increase this, you also need to change the model.\n",
        "        '--model',\n",
        "        '/content/model_2x.pth',  # 2x, 3x and 4x are available from the repo above\n",
        "        '--image',\n",
        "        filename,\n",
        "        '--cuda'\n",
        "    ]\n",
        "    print(f'Upscaling frame {i}')\n",
        "\n",
        "    process = subprocess.Popen(cmd, cwd=f'{working_dir}/steps/')\n",
        "    stdout, stderr = process.communicate()\n",
        "    if process.returncode != 0:\n",
        "        print(stderr)\n",
        "        print(\n",
        "            \"You may be able to avoid this error by backing up the frames,\"\n",
        "            \"restarting the notebook, and running only the video synthesis cells,\"\n",
        "            \"or by decreasing the resolution of the image generation steps. \"\n",
        "            \"If you restart the notebook, you will have to define the `filepath` manually\"\n",
        "            \"by adding `filepath = 'PATH_TO_THE_VIDEO'` to the beginning of this cell. \"\n",
        "            \"If these steps do not work, please post the traceback in the github.\"\n",
        "        )\n",
        "        raise RuntimeError(stderr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "imc4wXpLTlZu"
      },
      "source": [
        "# ビデオ作成\n",
        "フレームを使用してビデオを生成します。FPSの数、最初のフレーム、最後のフレームなどを変更できます。  \n",
        "この手順は、メモリ不足エラーが原因で失敗する可能性があります。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G8yGUysnTlZu"
      },
      "outputs": [],
      "source": [
        "# @title Create video\n",
        "# import subprocess in case this cell is run without the above cells\n",
        "import subprocess\n",
        "\n",
        "# Try to avoid OOM errors\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "init_frame = 1#@param {type:\"number\"} This is the frame where the video will start\n",
        "last_frame = 60#@param {type:\"number\"} You can change i to the number of the last frame you want to generate. It will raise an error if that number of frames does not exist.\n",
        "fps = 12#@param {type:\"number\"}\n",
        "\n",
        "try:\n",
        "    key_frames\n",
        "except NameError:\n",
        "    filename = \"video.mp4\"\n",
        "else:\n",
        "    if key_frames:\n",
        "        # key frame filename would be too long\n",
        "        filename = \"video.mp4\"\n",
        "    else:\n",
        "        filename = f\"{'_'.join(text_prompts).replace(' ', '')}.mp4\"\n",
        "filepath = f'{working_dir}/{filename}'\n",
        "\n",
        "frames = []\n",
        "# tqdm.write('Generating video...')\n",
        "try:\n",
        "    zoomed\n",
        "except NameError:\n",
        "    image_path = f'{working_dir}/steps/%04d.png'\n",
        "else:\n",
        "    image_path = f'{working_dir}/steps/zoomed_%04d.png'\n",
        "\n",
        "cmd = [\n",
        "    'ffmpeg',\n",
        "    '-y',\n",
        "    '-vcodec',\n",
        "    'png',\n",
        "    '-r',\n",
        "    str(fps),\n",
        "    '-start_number',\n",
        "    str(init_frame),\n",
        "    '-i',\n",
        "    image_path,\n",
        "    '-c:v',\n",
        "    'libx264',\n",
        "    '-frames:v',\n",
        "    str(last_frame-init_frame),\n",
        "    '-vf',\n",
        "    f'fps={fps}',\n",
        "    '-pix_fmt',\n",
        "    'yuv420p',\n",
        "    '-crf',\n",
        "    '17',\n",
        "    '-preset',\n",
        "    'veryslow',\n",
        "    filepath\n",
        "]\n",
        "\n",
        "process = subprocess.Popen(cmd, cwd=f'{working_dir}/steps/', stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
        "stdout, stderr = process.communicate()\n",
        "if process.returncode != 0:\n",
        "    print(stderr)\n",
        "    print(\n",
        "        \"You may be able to avoid this error by backing up the frames,\"\n",
        "        \"restarting the notebook, and running only the google drive/local connection and video synthesis cells,\"\n",
        "        \"or by decreasing the resolution of the image generation steps. \"\n",
        "        \"If these steps do not work, please post the traceback in the github.\"\n",
        "    )\n",
        "    raise RuntimeError(stderr)\n",
        "else:\n",
        "    print(\"The video is ready\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8_ae5H8UTlZv"
      },
      "outputs": [],
      "source": [
        "# @title Download video\n",
        "from google.colab import files\n",
        "files.download(filepath)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0M32NfoHTlZv"
      },
      "source": [
        "# スローモーション動画生成(Option)\n",
        "\n",
        "上記の手順の直後に実行すると、この手順のメモリが不足する可能性があります。   \n",
        "その場合は、ノートブックを再起動し、前の手順で保存したビデオのコピーをアップロードして（または、Googleドライブから取得して）、  \n",
        "下のセルを再度実行する前に、ビデオへのパスを使用して変数 `filepath`を定義してください"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_yPwypPRTlZv"
      },
      "outputs": [],
      "source": [
        "# @title Download Super-Slomo model\n",
        "!git clone -q --depth 1 https://github.com/avinashpaliwal/Super-SloMo.git\n",
        "from os.path import exists\n",
        "def download_from_google_drive(file_id, file_name):\n",
        "  # download a file from the Google Drive link\n",
        "  !rm -f ./cookie\n",
        "  !curl -c ./cookie -s -L \"https://drive.google.com/uc?export=download&id={file_id}\" > /dev/null\n",
        "  confirm_text = !awk '/download/ {print $NF}' ./cookie\n",
        "  confirm_text = confirm_text[0]\n",
        "  !curl -Lb ./cookie \"https://drive.google.com/uc?export=download&confirm={confirm_text}&id={file_id}\" -o {file_name}\n",
        "  \n",
        "pretrained_model = 'SuperSloMo.ckpt'\n",
        "if not exists(pretrained_model):\n",
        "  download_from_google_drive('1IvobLDbRiBgZr3ryCRrWL8xDbMZ-KnpF', pretrained_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fmyRhJeFTlZw"
      },
      "outputs": [],
      "source": [
        "# import subprocess in case this cell is run without the above cells\n",
        "import subprocess\n",
        "\n",
        "SLOW_MOTION_FACTOR = 3#@param {type:\"number\"}\n",
        "TARGET_FPS = 12#@param {type:\"number\"}\n",
        "\n",
        "cmd1 = [\n",
        "    'python',\n",
        "    'Super-SloMo/video_to_slomo.py',\n",
        "    '--checkpoint',\n",
        "    pretrained_model,\n",
        "    '--video',\n",
        "    filepath,\n",
        "    '--sf',\n",
        "    str(SLOW_MOTION_FACTOR),\n",
        "    '--fps',\n",
        "    str(TARGET_FPS),\n",
        "    '--output',\n",
        "    f'{filepath}-slomo.mkv',\n",
        "]\n",
        "process = subprocess.Popen(cmd1, cwd=f'/content', stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
        "stdout, stderr = process.communicate()\n",
        "if process.returncode != 0:\n",
        "    raise RuntimeError(stderr)\n",
        "\n",
        "cmd2 = [\n",
        "    'ffmpeg',\n",
        "    '-i',\n",
        "    f'{filepath}-slomo.mkv',\n",
        "    '-pix_fmt',\n",
        "    'yuv420p',\n",
        "    '-crf',\n",
        "    '17',\n",
        "    '-preset',\n",
        "    'veryslow',\n",
        "    f'{filepath}-slomo.mp4',\n",
        "]\n",
        "\n",
        "process = subprocess.Popen(cmd2, cwd=f'/content', stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
        "stdout, stderr = process.communicate()\n",
        "if process.returncode != 0:\n",
        "    raise RuntimeError(stderr)\n",
        "    print(stderr)\n",
        "    print(\n",
        "        \"You may be able to avoid this error by backing up the frames,\"\n",
        "        \"restarting the notebook, and running only the video synthesis cells,\"\n",
        "        \"or by decreasing the resolution of the image generation steps. \"\n",
        "        \"If you restart the notebook, you will have to define the `filepath` manually\"\n",
        "        \"by adding `filepath = 'PATH_TO_THE_VIDEO'` to the beginning of this cell. \"\n",
        "        \"If these steps do not work, please post the traceback in the github.\"\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l1urRk9HTlZw"
      },
      "outputs": [],
      "source": [
        "# @title Download video\n",
        "from google.colab import files\n",
        "files.download(f'{filepath}-slomo.mp4')"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "orig_nbformat": 4,
    "colab": {
      "name": "VQGAN_CLIP_Animation_demo.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}